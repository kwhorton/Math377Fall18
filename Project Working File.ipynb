{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math 377 Fall 2018\n",
    "\n",
    "#### Name:\n",
    "#### Section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentation Statement:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Data Collection, Summarization, Inference and Prediction\n",
    "## Building a Naive Bayes Classifier\n",
    "\n",
    "This project is designed to cover many of the main ideas of the entire course. We will go into a little more detail with the data collection and prediction. Ultimately we want to predict if an email is spam. In the course of doing this, we will collect data, clean it up, work with string data, make a simple inference, and then build a naive bayes model from the ground up. We will then bring in the package `scikit-learn` to check our work and add extra functionality.\n",
    "\n",
    "By the end of project, you should know how to:\n",
    "\n",
    "1. Find and import data.\n",
    "2. Use regular expressions to edit string data.\n",
    "3. Determine if a word helps to identify an email as spam or not.\n",
    "4. Create a function to predict the type of email using the ideas of Bayesian Classification.\n",
    "5. Assess your model and propose improvements.\n",
    "\n",
    "**Advice.** Develop your answers incrementally. To perform a complicated table manipulation, break it up into steps, perform each step on a different line, give a new name to each result, and check that each intermediate result is what you expect by displaying it. You can add additional names or functions to the provided cells in order to organize your work. \n",
    "\n",
    "**Authorized Resources:** Anyone and anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Background Information \n",
    "\n",
    "There are a couple of reference papers that may be of interest to explore. The first is \"Better Bayesian Filtering\" by Paul Graham,http://bit.ly/1ycPbiy. The second is \"A Plan for Spam\" also by Paul Graham, http://bit.ly/1ycPcmA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Packages  \n",
    "\n",
    "To get started, load `datascience`, `numpy`, `plots`, and `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datascience\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import scikit-learn\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get Data\n",
    "\n",
    "We are going to use data from the [Apache SpamAssasian](https://spamassassin.apache.org/) website. In particular we want data from their public corpus, see the readme document at https://spamassassin.apache.org/old/publiccorpus/.\n",
    "\n",
    "We have provided you the data in a zip file, spam.zip, because the data on the website is in a `tar` and `bz2` zip format. There are functions in python that allow you to address these types of file, see https://docs.python.org/3/library/tarfile.html, but it is not worth the extra coding effort at this point.\n",
    "\n",
    "You should extract the zip file and place the three folders in a folder called data under the directory where you are running this notebook. The three folders are  \n",
    "easy_ham  \n",
    "spam  \n",
    "hard_ham  \n",
    "\n",
    "The files names are the message number and the MD5 checksum. This is not convenient nomenclature. To access the files we will use the `glob` package, https://docs.python.org/3/library/glob.html. We also use the `os` package to get the current working directory on your machine. Below is the code to open one file to view its contents. \n",
    "\n",
    "Note: using the `with` argument will automatically close the file upon completion of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob,re,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Brad.Warner\\\\Documents\\\\Classes\\\\Math 377\\\\Fall 2018\\\\data\\\\*\\\\*'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get working directory, assumed to be\n",
    "cwd = os.getcwd()\n",
    "path=cwd +'\\\\data\\\\*\\\\*'\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Brad.Warner\\\\Documents\\\\Classes\\\\Math 377\\\\Fall 2018\\\\data\\\\easy_ham\\\\0001.ea7e79d3153e7469e7a9c3e0af6a357e',\n",
       " 'C:\\\\Users\\\\Brad.Warner\\\\Documents\\\\Classes\\\\Math 377\\\\Fall 2018\\\\data\\\\easy_ham\\\\0002.b3120c4bcbf3101e661161ee7efcb8bf',\n",
       " 'C:\\\\Users\\\\Brad.Warner\\\\Documents\\\\Classes\\\\Math 377\\\\Fall 2018\\\\data\\\\easy_ham\\\\0003.acfc5ad94bbd27118a0d8685d18c89dd',\n",
       " 'C:\\\\Users\\\\Brad.Warner\\\\Documents\\\\Classes\\\\Math 377\\\\Fall 2018\\\\data\\\\easy_ham\\\\0004.e8d5727378ddde5c3be181df593f1712',\n",
       " 'C:\\\\Users\\\\Brad.Warner\\\\Documents\\\\Classes\\\\Math 377\\\\Fall 2018\\\\data\\\\easy_ham\\\\0005.8c3b9e9c0f3f183ddaf7592a11b99957',\n",
       " 'C:\\\\Users\\\\Brad.Warner\\\\Documents\\\\Classes\\\\Math 377\\\\Fall 2018\\\\data\\\\easy_ham\\\\0006.ee8b0dba12856155222be180ba122058',\n",
       " 'C:\\\\Users\\\\Brad.Warner\\\\Documents\\\\Classes\\\\Math 377\\\\Fall 2018\\\\data\\\\easy_ham\\\\0007.c75188382f64b090022fa3b095b020b0',\n",
       " 'C:\\\\Users\\\\Brad.Warner\\\\Documents\\\\Classes\\\\Math 377\\\\Fall 2018\\\\data\\\\easy_ham\\\\0008.20bc0b4ba2d99aae1c7098069f611a9b',\n",
       " 'C:\\\\Users\\\\Brad.Warner\\\\Documents\\\\Classes\\\\Math 377\\\\Fall 2018\\\\data\\\\easy_ham\\\\0009.435ae292d75abb1ca492dcc2d5cf1570',\n",
       " 'C:\\\\Users\\\\Brad.Warner\\\\Documents\\\\Classes\\\\Math 377\\\\Fall 2018\\\\data\\\\easy_ham\\\\0010.4996141de3f21e858c22f88231a9f463']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(path)[:10] #Returns list with all the files in it, we will look at first 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the first file and print out the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From exmh-workers-admin@redhat.com  Thu Aug 22 12:36:23 2002\n",
      "\n",
      "Return-Path: <exmh-workers-admin@example.com>\n",
      "\n",
      "Delivered-To: zzzz@localhost.netnoteinc.com\n",
      "\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\n",
      "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id D03E543C36\n",
      "\n",
      "\tfor <zzzz@localhost>; Thu, 22 Aug 2002 07:36:16 -0400 (EDT)\n",
      "\n",
      "Received: from phobos [127.0.0.1]\n",
      "\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\n",
      "\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 12:36:16 +0100 (IST)\n",
      "\n",
      "Received: from listman.example.com (listman.example.com [66.187.233.211]) by\n",
      "\n",
      "    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g7MBYrZ04811 for\n",
      "\n",
      "    <zzzz-exmh@example.com>; Thu, 22 Aug 2002 12:34:53 +0100\n",
      "\n",
      "Received: from listman.example.com (localhost.localdomain [127.0.0.1]) by\n",
      "\n",
      "    listman.redhat.com (Postfix) with ESMTP id 8386540858; Thu, 22 Aug 2002\n",
      "\n",
      "    07:35:02 -0400 (EDT)\n",
      "\n",
      "Delivered-To: exmh-workers@listman.example.com\n",
      "\n",
      "Received: from int-mx1.corp.example.com (int-mx1.corp.example.com\n",
      "\n",
      "    [172.16.52.254]) by listman.redhat.com (Postfix) with ESMTP id 10CF8406D7\n",
      "\n",
      "    for <exmh-workers@listman.redhat.com>; Thu, 22 Aug 2002 07:34:10 -0400\n",
      "\n",
      "    (EDT)\n",
      "\n",
      "Received: (from mail@localhost) by int-mx1.corp.example.com (8.11.6/8.11.6)\n",
      "\n",
      "    id g7MBY7g11259 for exmh-workers@listman.redhat.com; Thu, 22 Aug 2002\n",
      "\n",
      "    07:34:07 -0400\n",
      "\n",
      "Received: from mx1.example.com (mx1.example.com [172.16.48.31]) by\n",
      "\n",
      "    int-mx1.corp.redhat.com (8.11.6/8.11.6) with SMTP id g7MBY7Y11255 for\n",
      "\n",
      "    <exmh-workers@redhat.com>; Thu, 22 Aug 2002 07:34:07 -0400\n",
      "\n",
      "Received: from ratree.psu.ac.th ([202.28.97.6]) by mx1.example.com\n",
      "\n",
      "    (8.11.6/8.11.6) with SMTP id g7MBIhl25223 for <exmh-workers@redhat.com>;\n",
      "\n",
      "    Thu, 22 Aug 2002 07:18:55 -0400\n",
      "\n",
      "Received: from delta.cs.mu.OZ.AU (delta.coe.psu.ac.th [172.30.0.98]) by\n",
      "\n",
      "    ratree.psu.ac.th (8.11.6/8.11.6) with ESMTP id g7MBWel29762;\n",
      "\n",
      "    Thu, 22 Aug 2002 18:32:40 +0700 (ICT)\n",
      "\n",
      "Received: from munnari.OZ.AU (localhost [127.0.0.1]) by delta.cs.mu.OZ.AU\n",
      "\n",
      "    (8.11.6/8.11.6) with ESMTP id g7MBQPW13260; Thu, 22 Aug 2002 18:26:25\n",
      "\n",
      "    +0700 (ICT)\n",
      "\n",
      "From: Robert Elz <kre@munnari.OZ.AU>\n",
      "\n",
      "To: Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "\n",
      "Cc: exmh-workers@example.com\n",
      "\n",
      "Subject: Re: New Sequences Window\n",
      "\n",
      "In-Reply-To: <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "References: <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "    <1029882468.3116.TMDA@deepeddy.vircio.com> <9627.1029933001@munnari.OZ.AU>\n",
      "\n",
      "    <1029943066.26919.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "    <1029944441.398.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "MIME-Version: 1.0\n",
      "\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "\n",
      "Message-Id: <13258.1030015585@munnari.OZ.AU>\n",
      "\n",
      "X-Loop: exmh-workers@example.com\n",
      "\n",
      "Sender: exmh-workers-admin@example.com\n",
      "\n",
      "Errors-To: exmh-workers-admin@example.com\n",
      "\n",
      "X-Beenthere: exmh-workers@example.com\n",
      "\n",
      "X-Mailman-Version: 2.0.1\n",
      "\n",
      "Precedence: bulk\n",
      "\n",
      "List-Help: <mailto:exmh-workers-request@example.com?subject=help>\n",
      "\n",
      "List-Post: <mailto:exmh-workers@example.com>\n",
      "\n",
      "List-Subscribe: <https://listman.example.com/mailman/listinfo/exmh-workers>,\n",
      "\n",
      "    <mailto:exmh-workers-request@redhat.com?subject=subscribe>\n",
      "\n",
      "List-Id: Discussion list for EXMH developers <exmh-workers.example.com>\n",
      "\n",
      "List-Unsubscribe: <https://listman.example.com/mailman/listinfo/exmh-workers>,\n",
      "\n",
      "    <mailto:exmh-workers-request@redhat.com?subject=unsubscribe>\n",
      "\n",
      "List-Archive: <https://listman.example.com/mailman/private/exmh-workers/>\n",
      "\n",
      "Date: Thu, 22 Aug 2002 18:26:25 +0700\n",
      "\n",
      "\n",
      "\n",
      "    Date:        Wed, 21 Aug 2002 10:54:46 -0500\n",
      "\n",
      "    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "\n",
      "    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  | I can't reproduce this error.\n",
      "\n",
      "\n",
      "\n",
      "For me it is very repeatable... (like every time, without fail).\n",
      "\n",
      "\n",
      "\n",
      "This is the debug log of the pick happening ...\n",
      "\n",
      "\n",
      "\n",
      "18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\n",
      "\n",
      "18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\n",
      "\n",
      "18:19:04 Ftoc_PickMsgs {{1 hit}}\n",
      "\n",
      "18:19:04 Marking 1 hits\n",
      "\n",
      "18:19:04 tkerror: syntax error in expression \"int ...\n",
      "\n",
      "\n",
      "\n",
      "Note, if I run the pick command by hand ...\n",
      "\n",
      "\n",
      "\n",
      "delta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\n",
      "\n",
      "1 hit\n",
      "\n",
      "\n",
      "\n",
      "That's where the \"1 hit\" comes from (obviously).  The version of nmh I'm\n",
      "\n",
      "using is ...\n",
      "\n",
      "\n",
      "\n",
      "delta$ pick -version\n",
      "\n",
      "pick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\n",
      "\n",
      "\n",
      "\n",
      "And the relevant part of my .mh_profile ...\n",
      "\n",
      "\n",
      "\n",
      "delta$ mhparam pick\n",
      "\n",
      "-seq sel -list\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Since the pick command works, the sequence (actually, both of them, the\n",
      "\n",
      "one that's explicit on the command line, from the search popup, and the\n",
      "\n",
      "one that comes from .mh_profile) do get created.\n",
      "\n",
      "\n",
      "\n",
      "kre\n",
      "\n",
      "\n",
      "\n",
      "ps: this is still using the version of the code form a day ago, I haven't\n",
      "\n",
      "been able to reach the cvs repository today (local routing issue I think).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "\n",
      "Exmh-workers mailing list\n",
      "\n",
      "Exmh-workers@redhat.com\n",
      "\n",
      "https://listman.redhat.com/mailman/listinfo/exmh-workers\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(glob.glob(path)[0],'r') as file: #the 'r' tells python to read the file only.\n",
    "    for line in file:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a tremendous amount of information in each email. To keep this project in scope, let's just use the subject line as our predictor. We need to use string commands from the `re` package to extract the subject line from each file. We will create a list of the subject line, the predictor, and the class of the email, spam or not. We determine class from the folder name. \n",
    "\n",
    "The command `re.sub` requires a regular expression. We will use `^Subject` as this tells the function we want to find  at the start of the string, https://docs.python.org/2/library/re.html. The method `strip` takes out the white spaces at the beginning and end of the string.\n",
    "\n",
    "To read in the files we will use `latin1` codec because the default and standard `utf-8` produce an error. We could ignore the error as well. To see more information on this go to http://balusc.omnifaces.org/2009/05/unicode-how-to-get-characters-right.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for fn in glob.glob(path):\n",
    "    is_spam = \"ham\" not in fn\n",
    "    \n",
    "    with open(fn,'r',encoding = 'latin1') as file:\n",
    "        # could use errors='ignore' instead of coding\n",
    "        for line in file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                subject=re.sub(r\"^Subject: \",\"\",line).strip()\n",
    "                data.append((subject,is_spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out the first 10 elements of the data list gives an idea of the emails' subject lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Re: New Sequences Window', False),\n",
       " ('[zzzzteana] RE: Alexander', False),\n",
       " ('[zzzzteana] Moscow bomber', False),\n",
       " (\"[IRR] Klez: The Virus That  Won't Die\", False),\n",
       " ('Re: Insert signature', False),\n",
       " ('Re: [zzzzteana] Nothing like mama used to make', False),\n",
       " ('Re: [zzzzteana] Nothing like mama used to make', False),\n",
       " ('[zzzzteana] Playboy wants to go out with a bang', False),\n",
       " ('Re: [zzzzteana] Nothing like mama used to make', False),\n",
       " ('[zzzzteana] Meaningful sentences', False)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not sure what zzzzteana means and whether it is only associated with ham. This may be something we remove later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3423"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) # Number of emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(zip(*data))[1]) #Number of spam emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering  \n",
    "\n",
    "We are dealing with string data as our predictor. We first need to clean it up. The choices we make here will potentially have a big impact on the quality of the model. Ideally we would go back and test the sensitivity of our results to these choices.\n",
    "\n",
    "First we will make all the text lower case. This will ensure that words such as Free and free are viewed as equivalent. This may not be a good idea for spam as an all capitals word might be more indicative of spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'free'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the function we need.\n",
    "'FREE'.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to tokenize our string. This means to split the string into a list of words. This requires the use of regular expressions, https://docs.python.org/2/library/re.html. \n",
    "\n",
    "As an example, we will use the first ten subject lines of the data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['re', 'new', 'sequences', 'window']\n",
      "['zzzzteana', 're', 'alexander']\n",
      "['zzzzteana', 'moscow', 'bomber']\n",
      "['irr', 'klez', 'the', 'virus', 'that', \"won't\", 'die']\n",
      "['re', 'insert', 'signature']\n",
      "['re', 'zzzzteana', 'nothing', 'like', 'mama', 'used', 'to', 'make']\n",
      "['re', 'zzzzteana', 'nothing', 'like', 'mama', 'used', 'to', 'make']\n",
      "['zzzzteana', 'playboy', 'wants', 'to', 'go', 'out', 'with', 'a', 'bang']\n",
      "['re', 'zzzzteana', 'nothing', 'like', 'mama', 'used', 'to', 'make']\n",
      "['zzzzteana', 'meaningful', 'sentences']\n"
     ]
    }
   ],
   "source": [
    "for subject, is_spam in data[:10]:\n",
    "    print(re.findall(\"[a-z0-9']+\",subject.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is a common step in text preparation to remove common words, called stop words, for example the word the. These words are common and tend not to aid in prediction. \n",
    "\n",
    "An easy source for these words comes from the `nltk`, natural language toolkit, package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Getting the English stop words from nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Printing out the first eight stop words\n",
    "stop_words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['re', 'new', 'sequences', 'window']\n",
      "['new', 'sequences', 'window']\n",
      "['zzzzteana', 're', 'alexander']\n",
      "['zzzzteana', 'alexander']\n",
      "['zzzzteana', 'moscow', 'bomber']\n",
      "['zzzzteana', 'moscow', 'bomber']\n",
      "['irr', 'klez', 'the', 'virus', 'that', \"won't\", 'die']\n",
      "['irr', 'klez', 'virus', \"won't\", 'die']\n",
      "['re', 'insert', 'signature']\n",
      "['insert', 'signature']\n",
      "['re', 'zzzzteana', 'nothing', 'like', 'mama', 'used', 'to', 'make']\n",
      "['zzzzteana', 'nothing', 'like', 'mama', 'used', 'make']\n",
      "['re', 'zzzzteana', 'nothing', 'like', 'mama', 'used', 'to', 'make']\n",
      "['zzzzteana', 'nothing', 'like', 'mama', 'used', 'make']\n",
      "['zzzzteana', 'playboy', 'wants', 'to', 'go', 'out', 'with', 'a', 'bang']\n",
      "['zzzzteana', 'playboy', 'wants', 'go', 'with', 'bang']\n",
      "['re', 'zzzzteana', 'nothing', 'like', 'mama', 'used', 'to', 'make']\n",
      "['zzzzteana', 'nothing', 'like', 'mama', 'used', 'make']\n",
      "['zzzzteana', 'meaningful', 'sentences']\n",
      "['zzzzteana', 'meaningful', 'sentences']\n"
     ]
    }
   ],
   "source": [
    "for subject, is_spam in data[:10]:\n",
    "    print(re.findall(\"[a-z0-9']+\",subject.lower()))\n",
    "    words = re.findall(\"[a-z0-9']+\",subject.lower())\n",
    "    [words.remove(word) for word in words if word in stop_words]\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further, let's summarize the data. We want to count the total frequency of words in both the spam and ham data sets. Then print those out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function that will tokenize our data and remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token1(subject,sw):\n",
    "    words = re.findall(\"[a-z0-9']+\",subject.lower())\n",
    "    [words.remove(word) for word in words if word in sw]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zzzzteana', 'alexander']\n"
     ]
    }
   ],
   "source": [
    "print(token1(data[1][0],stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n",
    "http://localhost:8888/notebooks/Documents/Classes/Books/Stats/Python%20Data%20Science%20Handbook/PythonDataScienceHandbook-master/notebooks/05.05-Naive-Bayes.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Depth: Naive Bayes Classification\n",
    "\n",
    "The previous four sections have given a general overview of the concepts of machine learning.\n",
    "In this section and the ones that follow, we will be taking a closer look at several specific algorithms for supervised and unsupervised learning, starting here with naive Bayes classification.\n",
    "\n",
    "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets.\n",
    "Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem.\n",
    "This section will focus on an intuitive explanation of how naive Bayes classifiers work, followed by a couple examples of them in action on some datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Classification\n",
    "\n",
    "Naive Bayes classifiers are built on Bayesian classification methods.\n",
    "These rely on Bayes's theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities.\n",
    "In Bayesian classification, we're interested in finding the probability of a label given some observed features, which we can write as $P(L~|~{\\rm features})$.\n",
    "Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
    "\n",
    "$$\n",
    "P(L~|~{\\rm features}) = \\frac{P({\\rm features}~|~L)P(L)}{P({\\rm features})}\n",
    "$$\n",
    "\n",
    "If we are trying to decide between two labels—let's call them $L_1$ and $L_2$—then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
    "\n",
    "$$\n",
    "\\frac{P(L_1~|~{\\rm features})}{P(L_2~|~{\\rm features})} = \\frac{P({\\rm features}~|~L_1)}{P({\\rm features}~|~L_2)}\\frac{P(L_1)}{P(L_2)}\n",
    "$$\n",
    "\n",
    "All we need now is some model by which we can compute $P({\\rm features}~|~L_i)$ for each label.\n",
    "Such a model is called a *generative model* because it specifies the hypothetical random process that generates the data.\n",
    "Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier.\n",
    "The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.\n",
    "\n",
    "This is where the \"naive\" in \"naive Bayes\" comes in: if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.\n",
    "Different types of naive Bayes classifiers rest on different naive assumptions about the data, and we will examine a few of these in the following sections.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Naive Bayes\n",
    "\n",
    "Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model.\n",
    "That said, they have several advantages:\n",
    "\n",
    "- They are extremely fast for both training and prediction\n",
    "- They provide straightforward probabilistic prediction\n",
    "- They are often very easily interpretable\n",
    "- They have very few (if any) tunable parameters\n",
    "\n",
    "These advantages mean a naive Bayesian classifier is often a good choice as an initial baseline classification.\n",
    "If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem.\n",
    "If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform.\n",
    "\n",
    "Naive Bayes classifiers tend to perform especially well in one of the following situations:\n",
    "\n",
    "- When the naive assumptions actually match the data (very rare in practice)\n",
    "- For very well-separated categories, when model complexity is less important\n",
    "- For very high-dimensional data, when model complexity is less important\n",
    "\n",
    "The last two points seem distinct, but they actually are related: as the dimension of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in *every single dimension* to be close overall).\n",
    "This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information.\n",
    "For this reason, simplistic classifiers like naive Bayes tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
